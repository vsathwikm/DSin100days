{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Gradient Descent - Learning Rate\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "We saw that Gradient descent algorithm’s main objective is to minimise the cost function. It is one of the best optimisation algorithms to minimise errors (difference of actual value and predicted value).\n",
    "\n",
    "The goal is similar like the above operation that we did to find out a best fit of intercept line ‘y’ in the slope ‘m’. Using Gradient descent algorithm also, we will figure out a minimal cost function by applying various parameters for theta 0 and theta 1 and see the slope intercept until it reaches convergence.\n",
    "\n",
    "\n",
    "<br/>\n",
    "<div style='float: left;'><img src='../../../images/gradient_descent.png'></div>\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "There are two parameters in our cost function we can control: m (weight) and b (bias). Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.\n",
    "\n",
    "The cost function is given by:\n",
    "\n",
    "$$ f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2 $$\n",
    "\n",
    "The gradient Descent is given by :\n",
    "\n",
    "$\\begin{split}f'(m,b) =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{df}{dm}\\\\\n",
    "     \\frac{df}{db}\\\\\n",
    "    \\end{bmatrix}\n",
    "=\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n",
    "     \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n",
    "    \\end{bmatrix}\\end{split}$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0:          3.886,\n",
      "Theta1:          3.310\n",
      "Final cost/MSE:  3629.914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1892371837)\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 +3 * X+np.random.randn(100,1)\n",
    "def  cal_cost(theta,X,y):\n",
    "   \n",
    "    m = len(y)\n",
    "    \n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        prediction = np.dot(X,theta)\n",
    "        \n",
    "        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n",
    "        theta_history[it,:] =theta.T\n",
    "        cost_history[it]  = cal_cost(theta,X,y)\n",
    "        \n",
    "    return theta, cost_history, theta_history\n",
    "\n",
    "lr =0.01\n",
    "n_iter = 1000\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history,theta_history = gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "Just run the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Learning Rate\n",
    "\n",
    "In the above code, we have included lists to capture the number of iterations and the cost function of each iteration.\n",
    "\n",
    "Next, we will plot the two to find out how it changes as per iterations.\n",
    "\n",
    "Check how with small learning rates it takes long to converge to the solution whereas with with larger learning rates it is quicker.\n",
    "\n",
    "A note of caution the actual learning rate for your problem would depend on the data and there is no general formula to set it right. However there are sophisticated optimization algorithms which start with a larger learning rates and then slowly reduce the learning rate as we approach the solution.\n",
    "\n",
    "In the below plot, the cost function starts converging at around 65 iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3c55d0026049>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'J(Theta)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iterations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.set_ylabel('J(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')\n",
    "ax.plot(63,30000, 'ro')  \n",
    "ax.plot(100,10000, 'ro')  \n",
    "ax.plot(90,10000, 'r>')  \n",
    "ax.plot(80,10000, 'r>')  \n",
    "ax.plot(75,10000, 'r>')  \n",
    "ax.plot(70,10000, 'r>')  \n",
    "ax.plot(65,10000, 'r>') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "Just run the above code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal Learning Rate\n",
    "\n",
    "In the above code, we have fixed the Learning rate (lr) as 0.01. While this is a good starting point,\n",
    "we must try having a higher learning rate and lower learning rate. For a  very high learning rate the cost function may not converge at all. Then we must start a maximum learning rate at which the curve looks converging and then start decreasing the learning rate until it is optimum.\n",
    "\n",
    "For a very low learning rate, the cost function might take a very long time to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =[0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "n_iter = 1000\n",
    "\n",
    "for l in lr:\n",
    "\n",
    "    theta = np.random.randn(2,1)\n",
    "\n",
    "    X_b = np.c_[np.ones((len(X),1)),X]\n",
    "    theta,cost_history,theta_history = gradient_descent(X_b,y,theta,l,n_iter)\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    ax.set_title('J(Theta) for Learning Rate ' + str(l))\n",
    "    ax.set_ylabel('J(Theta)')\n",
    "    ax.set_xlabel('Iterations')\n",
    "    _=ax.plot(range(n_iter),cost_history,'b.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "Just run the code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "executed_sections": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
