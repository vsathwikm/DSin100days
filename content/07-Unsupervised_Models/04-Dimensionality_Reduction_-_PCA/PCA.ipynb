{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l11111"
    ]
   },
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "<br>\n",
    "Principal Component Analysis or PCA is a classical linear approach to dimensionality reduction. As baffling as it may sound, this is however a relatively simple concept. Let's say you have a huge dataset and you want to predict a target. The dataset contains an overwhelming number of variables to consider and you don't know which ones to pick. Do you understand the relationships between each variable? Having so many variables, can you ensure that you are not overfitting your model to your data?\n",
    "\n",
    "PCA helps you solve this problem, by reducing the dimensions of your feature space. By reducing the dimensions of your feature space, you have fewer relationships between variables to consider thereby speeding up your algorithm; Also you are less likely to overfit your model.\n",
    "\n",
    "As seen earlier, huge datsets have many columns to consider, making it hard to perform Machine Learning operations for Analysis and Prediction.\n",
    "1. Statistical methods count observations that occur in a given space\n",
    "2. As the dimensions increase, number of observations in space become sparser\n",
    "3. Any model/estimator would require more time to converge to the true value\n",
    "\n",
    "**Dimensionality Reduction** is a method of converting dataset with vast dimensions to one with lesser dimensions also minimizing the extent of information loss. It is an effective approach to downsize data.\n",
    "\n",
    "<image>\n",
    "    \n",
    "## 1. Approaches to Dimensionality Reduction\n",
    "\n",
    "**A) Feature Selection:** \n",
    "\n",
    "It filters out irrelevant or redundant features from your dataset and chooses an optimal set of features! Only a subset of the original features are selected. It can be either\n",
    "- unsupervised (e.g. Variance Thresholds) or \n",
    "- supervised (e.g. Genetic Algorithms)\n",
    "\n",
    "**B) Feature Extraction:** \n",
    "\n",
    "It creates a new, smaller set of features by mapping original high-dimensional data onto a low-dimesnional space. The difference between Feature Selection and Feature Extraction is that Feature Selection keeps a subset of the original features while Feature Extraction creates new ones. It can be either\n",
    "- unsupervised (i.e. **PCA**) - maximizes the variance of the data\n",
    "- supervised (i.e. LDA) - maximizes the separation between different classes (*LDA stands for __Linear Discriminant Analysis__*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "l11111"
    ]
   },
   "outputs": [],
   "source": [
    "# No exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l22222"
    ]
   },
   "source": [
    "## 2. PCA\n",
    "\n",
    "PCA is an unsupervised algorithm that converts a set of observations of possibly correlated variables into a set which is a combination of original features. The new set of values are linearly uncorrelated variables called **principal components**.\n",
    "\n",
    "All dimension reduction techniques involve some degree of information loss. The goal of PCA is to reduce dimensionality while retaining as much variation as possible in the dataset!\n",
    "\n",
    "__Applications of PCA:__\n",
    "* Easing visualization of a huge dataset: (by reducing the dimensions of feature space)\n",
    "* Speeding up Machine Learning algorithms: (with reduced number of columns, ML algorithms can run a lot more faster)\n",
    "\n",
    "## 3. PCA Approach\n",
    "\n",
    "1. Standardize the data\n",
    "2. Calculate the covariance matrix\n",
    "3. Perform eigen decomposition on the covariance matrix\n",
    "4. Sort eigenvalues in descending order and choose k Eigenvectors that correspond to the k largest Eigenvalues (where **k** is the number of dimensions of the new feature subspace)\n",
    "5. Construct the projection matrix W from the selected **k** eigenvectors\n",
    "6. Project the data into into the subspace **W**\n",
    "\n",
    "<br>\n",
    "Let's see in detail all these steps using the famous \"Iris\" dataset.\n",
    "\n",
    "Loading \"Iris\" dataset:\n",
    "``` python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "raw = load_iris()\n",
    "iris = pd.DataFrame(raw.data, columns=raw.feature_names)\n",
    "iris['Target'] = raw['target']\n",
    "iris.head()\n",
    "```\n",
    "The Iris dataset:\n",
    "``` python\n",
    "      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  Target\n",
    "0                5.1               3.5                1.4               0.2       0\n",
    "1                4.9               3.0                1.4               0.2       0\n",
    "2                4.7               3.2                1.3               0.2       0\n",
    "3                4.6               3.1                1.5               0.2       0\n",
    "4                5.0               3.6                1.4               0.2       0\n",
    "\n",
    "```\n",
    "Clearly we can see that, the Iris dataset has four features:\n",
    "> 1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "and one 'Target' column which defines the target species that can be either 'setosa','versicolor' or 'virginica'.\n",
    "\n",
    "### Loading data:\n",
    "Now, we load **'tips'** dataset from seaborn and use this dataset to perfrom PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "l22222"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      "total_bill    244 non-null float64\n",
      "tip           244 non-null float64\n",
      "sex           244 non-null category\n",
      "smoker        244 non-null category\n",
      "day           244 non-null category\n",
      "time          244 non-null category\n",
      "size          244 non-null int64\n",
      "dtypes: category(4), float64(2), int64(1)\n",
      "memory usage: 7.2 KB\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "raw = sns.load_dataset('tips')\n",
    "raw.info()\n",
    "\n",
    "# Click continue to proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "Click continue to proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l33333"
    ]
   },
   "source": [
    "### 3.1 Standardize the data\n",
    "<br>\n",
    "\n",
    "**Why do we need to standardize data?**<br>\n",
    "\n",
    "Given a dataset, variables measured at different scales do not contribute equally to analysis. For example, in boundary detection, a variable that ranges between 0 and 100 will outweigh a variable that ranges between 0 and 1. In such a case, how do you know when an outlier occurs? Does it make sense to compare these variables ‘as is’ in a multivariate analysis?\n",
    "\n",
    "This is where Standardization comes into play. In simpler terms, Standardization allows us to use one single distribution to compare multi-variate data (say apples to oranges or bananas to grapes). Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data. Standardization rescales data to have a mean (μ) of 0 and standard deviation (σ) of 1 (unit variance). Lets understand the terms: \n",
    "\n",
    "> **a) Mean (μ):**\n",
    "    Average of a set of data <br>\n",
    "\n",
    "$$Mean_x = \\frac {\\sum x}{n}$$\n",
    "    \n",
    ">  **b) Variance (σ²):**\n",
    "    The average of the squared differences from the mean <br>\n",
    "$${S_x}^2 = \\frac {\\sum (x - \\bar{x})^2}{n -1}$$\n",
    "\n",
    ">  **c) Standard Deviation (σ):**\n",
    "    Square root of the variance\n",
    "$$ S = \\sqrt \\frac {\\sum (x - \\bar{x})^2}{n -1}$$\n",
    "\n",
    "The *scikit-learn* library has inbuilt function *StandardScaler()* that could standardize the data.\n",
    "``` python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = iris.drop('Target', axis=1)\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "```\n",
    "<br>\n",
    "\n",
    "### Exercise\n",
    "Let's say we want to use all the inforamtion from the **tips** dataset and predict the *total_bill*. Let's keep *total_bill* as the target variable. Using 'tips' variable, leave out the target and use StandardScaler function to standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "l33333"
    ]
   },
   "outputs": [],
   "source": [
    "#Converting categorical values into numerical values\n",
    "tips = raw.copy()\n",
    "cols_to_transform = ['sex','smoker','day','time']\n",
    "for x in cols_to_transform:\n",
    "    tips[x] = raw[x].cat.codes\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "X = tips.drop('total_bill', axis=1)\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l44444"
    ]
   },
   "source": [
    "### 3.2 Calculate the covariance matrix\n",
    "\n",
    "The covariance matrix Σ, is a d×d matrix where each element represents the covariance between two features. **Covariance** is a measure of how changes in one variable are associated with changes in a second variable. It’s similar to variance, but where variance tells you how a single variable varies, Covariance tells you how two variables vary together.\n",
    "\n",
    "<img src=\"../../../images/pca-covar_matrix.PNG\">\n",
    "\n",
    "Let's see how we can calculate the co-variance matrix using *numpy* library\n",
    "``` python\n",
    "import numpy as np\n",
    "from numpy import cov\n",
    "\n",
    "X_mean = np.mean(X_std, axis=0)\n",
    "cv_mat = cov((X_std - X_mean).T)\n",
    "print('Covariance matrix \\n', cv_mat)\n",
    "\n",
    ">>>\n",
    "Covariance matrix \n",
    " [[ 1.00671141 -0.11010327  0.87760486  0.82344326]\n",
    " [-0.11010327  1.00671141 -0.42333835 -0.358937  ]\n",
    " [ 0.87760486 -0.42333835  1.00671141  0.96921855]\n",
    " [ 0.82344326 -0.358937    0.96921855  1.00671141]]\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "Calculate the covariance matrix of **tips** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "l44444"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import cov\n",
    "\n",
    "#Add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "X_mean = np.mean(X_std, axis=0)\n",
    "cv_mat = cov((X_std - X_mean).T)\n",
    "print('Covariance matrix: \\n', cv_mat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l55555"
    ]
   },
   "source": [
    "\n",
    "### 3.3 Perform eigendecomposition on the covariance matrix\n",
    "\n",
    "The eigenvectors and eigenvalues of a covariance matrix form the \"core\" of a PCA: The eigenvectors represent the directions or principal components, and the eigenvalues determine their magnitude.\n",
    "\n",
    "Given:\n",
    "> **λ:** scalar <br>\n",
    "**I;** identity matrix <br>\n",
    "**A:** non-singular matrix (our covariance matrix) <br>\n",
    "\n",
    "**a) Eigenvalues:**\n",
    "\n",
    "The values of **λ** for which **det (A-λI)=0** are called eigenvalues\n",
    "\n",
    "**b) Eigenvectors:**\n",
    "\n",
    "A vector ** *v* ** for which **(A-λI)*v*=0** is called eigenvector\n",
    "<br>\n",
    "\n",
    "Let's see how we can calculate the eigenvectors and eigenvalues using *numpy* library\n",
    "``` python\n",
    "from numpy.linalg import eig\n",
    "\n",
    "e_values, e_vectors = eig(cv_mat)\n",
    "print('Eigenvectors \\n', e_vectors)\n",
    "print('\\nEigenvalues \\n', e_values)\n",
    "\n",
    ">>>\n",
    "Eigenvectors \n",
    " [[ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
    " [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
    " [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
    " [ 0.56561105 -0.06541577  0.6338014   0.52354627]]\n",
    "\n",
    "Eigenvalues \n",
    " [2.93035378 0.92740362 0.14834223 0.02074601]\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Note:** Instead of Eigen Decomposition, other matrix decomposition methods can be used such as **Singular-Value Decomposition (SVD)**. Most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency.\n",
    "<br>\n",
    "\n",
    "### Exercise\n",
    "Compute the eigenvectors and eigenvalues of **tips** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "l55555"
    ]
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "#Add your code here\n",
    "\n",
    "\n",
    "#print('Eigenvectors: \\n', ******)\n",
    "#print('\\nEigenvalues: \\n', ******)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "e_values, e_vectors = eig(cv_mat)\n",
    "print('Eigenvectors: \\n', e_vectors)\n",
    "print('\\nEigenvalues: \\n', e_values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l66666"
    ]
   },
   "source": [
    "\n",
    "### 3.4 Choose **k** eigenvectors\n",
    "\n",
    "This is the most important step in PCA approach as the eigenvectors define the directions of the new axis. Therefore we have to carefully choose k eigenvectors that correspond to the k largest Eigenvalues (where **k** is the number of dimensions of the new feature subspace)\n",
    "\n",
    "**Steps in choosing eigenvectors:**<br>\n",
    "1.Rank the eigenvalues from highest to lowest: This is because the eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data and they can be dropped.\n",
    "\n",
    "Now let's create the EigenPairs and sort them based on the eigenvalues from highest to lowest.\n",
    "\n",
    "``` python\n",
    "e_pairs = [(np.abs(e_values[i]), e_vectors[:,i]) for i in range(len(e_values))]\n",
    "e_pairs.sort(reverse=True)\n",
    "e_pairs\n",
    "\n",
    ">>>\n",
    "[(2.930353775589318,\n",
    "  array([ 0.52237162, -0.26335492,  0.58125401,  0.56561105])),\n",
    " (0.927403621517341,\n",
    "  array([-0.37231836, -0.92555649, -0.02109478, -0.06541577])),\n",
    " (0.14834222648164,\n",
    "  array([-0.72101681,  0.24203288,  0.14089226,  0.6338014 ])),\n",
    " (0.0207460139955958,\n",
    "  array([ 0.26199559, -0.12413481, -0.80115427,  0.52354627]))]\n",
    "```\n",
    "<br>\n",
    "\n",
    "2.Decide on the value of k : There is no single golden method to select k, there are multiple ways. It all depends on what works well for your model. One way is to use a **Scree plot**. The most commonly used way is plotting **Percentage of Cumulative Variance** and visually choosing **k**.\n",
    "\n",
    "<img src=\"../../../images/pca-choosing_k.png\">\n",
    "\n",
    "Lets see the plot of Percentage of Cumulative Variance:\n",
    "    First we calculate the sum of Variance, and then we calculate the percentage of variance of each principal component (Explained Variance). Later we calculate the Cumulative Variance as shown below.\n",
    "    \n",
    "``` python\n",
    "tot_var = sum(e_values)\n",
    "exp_var_percent = [(i / tot_var)*100 for i in e_values]\n",
    "cum_var = np.cumsum(exp_var_percent)\n",
    "print('Explained Variance \\n', exp_var_percent)\n",
    "print('\\nCumulative Variance \\n', cum_var)\n",
    "\n",
    ">>>\n",
    "Explained Variance \n",
    " [72.77045209380135, 23.03052326768062, 3.6838319576273912, 0.5151926808906288]\n",
    "\n",
    "Cumulative Variance \n",
    " [ 72.77045209  95.80097536  99.48480732 100.        ]\n",
    "```\n",
    "\n",
    "This information would make more sense when we visualize graphically.\n",
    "\n",
    "``` python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = ['PC %s' %i for i in range(1,len(e_values)+1)]\n",
    "y_1 = exp_var_percent\n",
    "y_2 = cum_var\n",
    "plt.bar(x, y_1)\n",
    "plt.scatter(x, y_2)\n",
    "plt.plot(x, y_2)\n",
    "for i,j in zip(x,y_1):\n",
    "    plt.annotate(str(round(j,2)),xy=(i,j+0.5))\n",
    "```\n",
    "\n",
    "<img src=\"../../../images/pca-cum_var_plot.png\">\n",
    "\n",
    "\n",
    "From the above plot it is evident that most of the variance is explained by the first principal component(72.77% of the variance) and the second principal component(23% of the variance) alone. Together they contain 95.8% of the information. The third and the fourth principal components can be safely dropped wihtout loosing to much information. Since we need just two principal components to explain the information, the value of **k** now is 2.\n",
    "<br>\n",
    "\n",
    "### Exercise\n",
    "Decide on the value of k for tips dataset. First, calculate the Explained variance and Cumulative variance. Second, plot the principal components against the Percentage of Cumulative Variance. From the plot visually choose **k**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "l66666"
    ]
   },
   "outputs": [],
   "source": [
    "e_pairs = [(np.abs(e_values[i]), e_vectors[:,i]) for i in range(len(e_values))]\n",
    "e_pairs.sort(reverse=True)\n",
    "\n",
    "# Add your code for calulation here\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Add your code for plot here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "tot_var = sum(e_values)\n",
    "exp_var_percent = [(i / tot_var)*100 for i in e_values]\n",
    "cum_var = np.cumsum(exp_var_percent)\n",
    "\n",
    "x = ['PC %s' %i for i in range(1,len(e_values)+1)]\n",
    "y_1 = exp_var_percent\n",
    "y_2 = cum_var\n",
    "plt.bar(x, y_1)\n",
    "plt.scatter(x, y_2)\n",
    "plt.plot(x, y_2)\n",
    "for i,j in zip(x,y_1):\n",
    "    plt.annotate(str(round(j,2)),xy=(i,j+0.5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l77777"
    ]
   },
   "source": [
    "\n",
    "### 3.5 Construct the projection matrix **W**\n",
    "\n",
    "Once the ** *k* ** value is choosen the, a subspace is created so that data can be projected into this subspace. This subspace is also called the **Projection Matrix**, which is basically a matrix of our concatenated top k eigenvectors.\n",
    "\n",
    "For Iris dataset, by choosing the top 2 eigenvectors, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace.\n",
    "\n",
    "``` python\n",
    "matrix_w = np.hstack((e_pairs[0][1].reshape(4,1), \n",
    "                      e_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix W:\\n', matrix_w)\n",
    "\n",
    ">>> Matrix W:\n",
    "[[ 0.52237162 -0.37231836]\n",
    " [-0.26335492 -0.92555649]\n",
    " [ 0.58125401 -0.02109478]\n",
    " [ 0.56561105 -0.06541577]]\n",
    "```\n",
    "<br>\n",
    "\n",
    "### Exercise\n",
    "Looking at the plot you have got in the previous exercise, it can be seen that most of the variance is explained by the first (35.13%), second (23%), fifth (16.69%) and sixth 14.99%) principal components alone. Together they contain around 90% of the information. Hence we choose ** *k*=4 ** for our problem.\n",
    "\n",
    "Now create the projection matrix of the top ** *k*=4 ** eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "l77777"
    ]
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "#matrix_w = \n",
    "#print('Matrix W:\\n', matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "matrix_w = np.hstack((e_pairs[0][1].reshape(6,1), \n",
    "                      e_pairs[1][1].reshape(6,1),\n",
    "                      e_pairs[4][1].reshape(6,1),\n",
    "                      e_pairs[5][1].reshape(6,1),))\n",
    "print('Matrix W:\\n', matrix_w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l88888"
    ]
   },
   "source": [
    "\n",
    "### 3.6 Project the data into into the subspace **W**\n",
    "\n",
    "Once the projection matrix is created, then the data can be transformed onto the new subspace via *matrix multiplication* as given by the equation: <br>\n",
    "**Y = X × W**\n",
    "\n",
    "``` python\n",
    "Y = X_std.dot(matrix_w)\n",
    "```\n",
    "\n",
    "In our example we have reduced the subspace from 4-dimensions to 2-dimensions. Let's visualize the distribution of our targets in our datset using *matplotlib*\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "c_map = {0: 'b', 1: 'r', 2:'g'}\n",
    "for i in range(0,len(Y)):\n",
    "    ax.scatter(Y[i,0],Y[i,1],c=c_map[iris.iloc[i]['Target']])\n",
    "\n",
    "plt.legend(['setosa','versicolor','virginica'])\n",
    "```\n",
    "\n",
    "<img src=\"../../../images/pca-reduced_subspace.png\">\n",
    "\n",
    "### Exercise\n",
    "Project the data into the subspace you created in the previous exercise. (No plot is required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "l88888"
    ]
   },
   "outputs": [],
   "source": [
    "# Add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "Y = X_std.dot(matrix_w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l99999"
    ]
   },
   "source": [
    "## 4. PCA in scikit-learn\n",
    "\n",
    "As we have seen above, the PCA approach using *numpy* library involves a lot of effort and time. This can be avoided by using the *scikit-learn* library as it has a predefined function that can handle PCA for us. The advantage is that once a projection is calculated, it can be applied to new data again and again quite easily.\n",
    "\n",
    "For example we can observe the distribution of our targets in the same datset as used above using *scikit-learn*\n",
    "\n",
    "``` python\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "c_map = {0: 'b', 1: 'r', 2:'g'}\n",
    "for i in range(0,len(Y_sklearn)):\n",
    "    ax.scatter(Y_sklearn[i,0],Y_sklearn[i,1],c=c_map[iris.iloc[i]['Target']])\n",
    "\n",
    "plt.legend(['setosa','versicolor','virginica'])\n",
    "```\n",
    "\n",
    "<img src=\"../../../images/pca-using_sklearn.png\">\n",
    "\n",
    "### Exercise\n",
    "Now, use **PCA()** function from **sklearn** to perform PCA on tips dataset. (No plot is required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "l99999"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "sklearn_pca = sklearnPCA(n_components=4)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
