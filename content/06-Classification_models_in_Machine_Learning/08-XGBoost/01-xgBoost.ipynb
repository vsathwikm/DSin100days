{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Library\n",
    "\n",
    "XGBoost is an implementation of decision tree ensembles. This tree ensemble model consists of a set of classification and regression trees (CART). A CART is a bit different from decision trees, in which the leaf only contains decision values. XGboost provides these advantages:\n",
    "\n",
    "Speed and performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.\n",
    "\n",
    "parallelizable Algorithms : Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPUâ€™s and across networks of computers making it feasible to train on very large datasets as well.\n",
    "\n",
    "Performance : It has shown better performance on a variety of machine learning benchmark datasets.\n",
    "\n",
    "Tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n",
    "\n",
    "\n",
    "### XGBoost in action\n",
    "\n",
    "We can use the XGBoost by importing xgboost as xgb.\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('../../../data/agaricus.train')\n",
    "dtest = xgb.DMatrix('../../../data/agaricus.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)                \n",
    "\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Import XGBoost library as xgb and use above dataset to train xgb. Print preds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:29:10] 6513x127 matrix with 143286 entries loaded from ../../../data/agaricus.train\n",
      "[11:29:10] 1611x127 matrix with 35442 entries loaded from ../../../data/agaricus.test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.28583017, 0.9239239 , 0.28583017, ..., 0.9239239 , 0.05169873,\n",
       "       0.9239239 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('../../../data/agaricus.train')\n",
    "dtest = xgb.DMatrix('../../../data/agaricus.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "Hit run on the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try regression using XGB with Boston  Housing data.\n",
    "\n",
    "\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "In this exercise, you are required to create a xgBoost for predicting Boston Housing MEDV, fit the dataset. Use kfold validation to split data before fitting model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Housing: regression\n",
      "22.247195111946144\n",
      "9.914663303584955\n",
      "Parameter optimization\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "0.5984879606490934\n",
      "{'max_depth': 4, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, classification_report\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "print(\"Boston Housing: regression\")\n",
    "boston = load_boston()\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "xgbr_params = {'objective':'reg:squarederror'}\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBRegressor(**xgbr_params).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(mean_squared_error(actuals, predictions))\n",
    "\n",
    "print(\"Parameter optimization\")\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "xgb_model = xgb.XGBRegressor(**xgbr_params)\n",
    "clf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2,4,6],\n",
    "                    'n_estimators': [50,100,200]}, verbose=1)\n",
    "clf.fit(X,y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution code\n",
    "\n",
    "Hit run on the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GridSearch with XGBoost\n",
    "\n",
    "Let us do GridSearchCV for tuning parameters for Regression.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Run GridSearchCV on xgBoost to fine tune parameters. Use n_estimators from 100 to 1000 and max_depth from 2 to 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "             'n_estimators' : np.arange(100, 1001, 100),\n",
    "             'max_depth' : np.arange(2, 6)\n",
    "             }\n",
    "\n",
    "xgbr_params = {'objective':'reg:squarederror','n_jobs':-1,'random_state':4444,'min_child_weight':1,\n",
    "              'eta':0.3,'subsample':0.8,'gamma':0.5,'colsample_bytree':0.8}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "reg = GridSearchCV(xgb.XGBRegressor(**xgbr_params)\n",
    "                              ,param_grid=cv_params,scoring='r2',cv=5,n_jobs=-1,return_train_score=True, verbose=3)\n",
    "\n",
    "reg.fit(X,y)\n",
    "print(reg.best_score_)\n",
    "print(reg.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Tuning in XGBoost\n",
    "\n",
    "Most of parameters in XGBoost are about bias variance tradeoff. The best model should trade the model complexity against accuracy of prediction.\n",
    "\n",
    "Two scenarios are provided as per XGBoost documentation.\n",
    "\n",
    "### Handling Overfitting\n",
    "\n",
    "When you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\n",
    "\n",
    "There are in general two ways that you can control overfitting in XGBoost:\n",
    "\n",
    "The first way is to directly control model complexity.\n",
    "This includes max_depth, min_child_weight and gamma.\n",
    "The second way is to add randomness to make training robust to noise.\n",
    "This includes subsample and colsample_bytree.\n",
    "You can also reduce stepsize eta. Remember to increase num_round when you do so.\n",
    "\n",
    "### Working with Imbalanced Dataset\n",
    "\n",
    "An imbalanced dataset can affect the training of XGBoost model, and there are two ways to improve it.\n",
    "\n",
    "<li>If you care only about the overall performance metric (AUC) of your prediction\n",
    "    \n",
    "Balance the positive and negative weights via scale_pos_weight and use AUC for evaluation\n",
    "\n",
    "<li>If you care about predicting the right probability\n",
    "\n",
    "In such a case, you cannot re-balance the dataset,Set parameter max_delta_step to a finite number (say 1) to help convergence\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Use XGBoost classifier on Carseats dataset. Predict binary class of High is 0 or 1 using XGB after tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 12 columns):\n",
      "Sales          400 non-null float64\n",
      "CompPrice      400 non-null int64\n",
      "Income         400 non-null int64\n",
      "Advertising    400 non-null int64\n",
      "Population     400 non-null int64\n",
      "Price          400 non-null int64\n",
      "ShelveLoc      400 non-null int64\n",
      "Age            400 non-null int64\n",
      "Education      400 non-null int64\n",
      "Urban          400 non-null int64\n",
      "US             400 non-null int64\n",
      "High           400 non-null int64\n",
      "dtypes: float64(1), int64(11)\n",
      "memory usage: 37.6 KB\n",
      "Test accuracy is 0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mani/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df3 = pd.read_csv('../../../data/Carseats.csv').drop('Unnamed: 0', axis=1)\n",
    "df3.head()\n",
    "df3['High'] = df3.Sales.map(lambda x: 1 if x>8 else 0)\n",
    "df3.ShelveLoc = pd.factorize(df3.ShelveLoc)[0]\n",
    "\n",
    "df3.Urban = df3.Urban.map({'No':0, 'Yes':1})\n",
    "df3.US = df3.US.map({'No':0, 'Yes':1})\n",
    "df3.info()\n",
    "X = df3.drop(['Sales', 'High'], axis=1)\n",
    "y = df3.High\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "xgbr_params = {'max_depth':2,  'silent':1, 'objective':'binary:logistic','n_jobs':-1,'random_state':4444,\n",
    "               'min_child_weight':1,\n",
    "              'eta':0.3,'subsample':0.8,'gamma':0.5,'colsample_bytree':0.8}\n",
    "clf = xgb.XGBClassifier(**xgbr_params)\n",
    "clf.fit(X,y)\n",
    "print('Test accuracy is', clf.score(X_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "Hit run on the above"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
