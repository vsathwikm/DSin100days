{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "As a recap of Decision Trees,  we learnt that it is a computational model that contains a set of if-then-else decisions to classify data and its similar to program flow diagrams. We also learnt that by ensemble methods, we can improve the performance of Trees using bagging and random forest. Ensemble Models or Ensemble Learning is the method of combining many weaker learning models together in order to provide better prediction capability. These learning models may run parallelly or sequentially and they may learn from each other, thus compounding learning in certain ensemble techniques.\n",
    "\n",
    "Boosting is an ensemble algorithm designed to improve accuracy of other Machine Learning algorithms similar to Bagging, except that in boosting the individual models such as Decision Trees are sequentially used in different splits. Unlike in bagging where there different splits of training dataset is used individually.\n",
    "\n",
    "Boosting does not involve bootstrapping, but instead each tree is fit with a modified version of the original training dataset sequentially. In boosting ensemble method, due to the approach of using the component decision trees sequentially, the resulting ensemble algorithm is boosted by each previous step.\n",
    "\n",
    "## Boosted Trees\n",
    "\n",
    "The Boosted Trees Model is an ensemble model for DecisionTrees that makes predictions by combining decisions from a sequence of base models. For boosted trees model, each base classifier is a simple decision tree. This is typically done by using Gradient Boosting called GBT\n",
    "\n",
    "The idea behind gradient boosted tree is to use the pattern in residuals for each Decision Tree similar to a residual in say Gradient Boosting in a LinearRegression. An initial Decision Tree is created using the full data. This initial model is further strengthened a to a strong model by modelling the residuals from the first iteration. This process is continued with subsequent residuals and so on. Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals. In essence, we are minimizing our loss function such that test loss reach its minima or removing the variance from the weaker models.\n",
    "\n",
    "\n",
    "### Boosted Tree in Regression Setting\n",
    "\n",
    "We can use the GradientBoostingRegressor from sklearn.ensemble as shown below.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)                 \n",
    "\n",
    "```\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "In this exercise, you are required to create a GradientBoostingRegressor for Boston Housing data, fit the dataset and print the test accuracy score and Test MSE. Ensure Train-Test split at 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution code\n",
    "\n",
    "```python\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "print('Test Accuracy is', gbr.score(X_test, y_test)) \n",
    "y_pred_train = gbr.predict(X_train)\n",
    "print('Train MSE ', mean_squared_error(y_train,y_pred_train))\n",
    "y_pred_test = gbr.predict(X_test)\n",
    "print('Test MSE ', mean_squared_error(y_test,y_pred_test))  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Boosted Tree in Classification Setting\n",
    "\n",
    "We can use the GradientBoostingClassifier from sklearn.ensemble as shown below.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)                 \n",
    "\n",
    "```\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "In this exercise, you are required to create a GradientBoostingClassifier for Carseats data, fit the dataset and print the test accuracy score. Ensure Train-Test split at 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 12 columns):\n",
      "Sales          400 non-null float64\n",
      "CompPrice      400 non-null int64\n",
      "Income         400 non-null int64\n",
      "Advertising    400 non-null int64\n",
      "Population     400 non-null int64\n",
      "Price          400 non-null int64\n",
      "ShelveLoc      400 non-null int64\n",
      "Age            400 non-null int64\n",
      "Education      400 non-null int64\n",
      "Urban          400 non-null int64\n",
      "US             400 non-null int64\n",
      "High           400 non-null int64\n",
      "dtypes: float64(1), int64(11)\n",
      "memory usage: 37.6 KB\n",
      "Train score for depth=1 is  0.885\n",
      "Test score for depth=1 is 0.76\n",
      "Train score for depth=2 is  0.955\n",
      "Test score for depth=2 is 0.84\n",
      "Train score for depth=3 is  1.0\n",
      "Test score for depth=3 is 0.84\n",
      "Train score for depth=4 is  1.0\n",
      "Test score for depth=4 is 0.795\n",
      "Train score for depth=5 is  1.0\n",
      "Test score for depth=5 is 0.795\n",
      "Train score for depth=6 is  1.0\n",
      "Test score for depth=6 is 0.79\n",
      "Train score for depth=7 is  1.0\n",
      "Test score for depth=7 is 0.77\n",
      "Train score for depth=8 is  1.0\n",
      "Test score for depth=8 is 0.73\n",
      "Train score for depth=9 is  1.0\n",
      "Test score for depth=9 is 0.725\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df3 = pd.read_csv('../../../data/Carseats.csv').drop('Unnamed: 0', axis=1)\n",
    "df3.head()\n",
    "df3['High'] = df3.Sales.map(lambda x: 1 if x>8 else 0)\n",
    "df3.ShelveLoc = pd.factorize(df3.ShelveLoc)[0]\n",
    "\n",
    "df3.Urban = df3.Urban.map({'No':0, 'Yes':1})\n",
    "df3.US = df3.US.map({'No':0, 'Yes':1})\n",
    "X = df3.drop(['Sales', 'High'], axis=1)\n",
    "y = df3.High\n",
    "df3.info()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "clf = GradientBoostingClassifier(max_depth=3)\n",
    "\n",
    "for i in range(1,10):\n",
    "    clf = GradientBoostingClassifier(max_depth=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    print('Train score for depth={} is '.format(i), clf.score(X_train, y_train))\n",
    "    print('Test score for depth={} is'.format(i), clf.score(X_test, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution code\n",
    "\n",
    "Hit run on the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fine Tuning the Boosted Tree\n",
    "\n",
    "The Gradient Boosted Trees model has many tuning parameters. \n",
    "\n",
    "<b>n_estimators : </b> Controls the number of trees in the final model. Usually the more trees, the higher accuracy. However, both the training and prediction time also grows linearly in the number of trees. You can choose max_iterations to be large and fit your computation budget.\n",
    "\n",
    "<b>max_depth</b> Restricts the depth of each individual tree to prevent overfitting.\n",
    "\n",
    "<b>min_weight_fraction_leaf</b>  In general,  You can then set min_weight_fraction_leaf to be a reasonable value around (#instances/1000), and tune max_depth. When you have more training instances, you can set max_depth to a higher value. When you find a large gap between the training loss and validation loss, a sign of overfitting, you may want to reduce depth, and increase min_weight_fraction_leaf.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "In this exercise, you are required to create afine tune the above GradientBoostingClassifier for Carseats data, to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your solution below by modifying the below code\n",
    "param = {'max_depth':2, 'n_estimators':100, 'min_weight_fraction_leaf':0.01}\n",
    "#clf = GradientBoostingClassifier(param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution code\n",
    "\n",
    "Hit run on the above code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
