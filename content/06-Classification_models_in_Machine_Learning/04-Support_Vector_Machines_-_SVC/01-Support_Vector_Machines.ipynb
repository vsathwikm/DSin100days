{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "s1",
     "content",
     "l1"
    ]
   },
   "source": [
    "# Support Vector Machines (SVMs)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Support Vector Machines are classifiers that can classify datasets by a introducing an optimal hyperplane between the multi-dimensional data points. An hyperplane is a multi-dimensional structure that extends a two-dimensional plane. If the datasets consists of two dimensional dataset, then an estimate line is fit that provides the best classification on the  dataset. By \"best classification\", it is to be noted that a plane that not necessarily provides perfect classification of all points in the training dataset but fits a criterion such that the line is farthest from all points. You can see from the figure below that a hyperplane classifies the dataset as shown.\n",
    "\n",
    "<img src=\"../../../images/SVM.PNG\" style=\"width:45vw\"> \n",
    "\n",
    "We shall use a plot_learning_curve function from sklearn:\n",
    "ref: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html .\n",
    "\n",
    "## Exercise\n",
    "\n",
    "* In the titanic dataset that has been cleaned, train a SVM classifier on the 'features' list provided below.\n",
    "* Perform a Train-Test split\n",
    "* Perform 10 fold cross-validation\n",
    "* Find out mean accuracy of the trained model and put the obtained accuracy in accuracy_train variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "s1",
     "ce",
     "l1"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "train_data = pd.read_csv(\"https://raw.githubusercontent.com/colaberry/data/master/Titanic/train_data.csv\")\n",
    "test_data = pd.read_csv(\"https://raw.githubusercontent.com/colaberry/data/master/Titanic/test_data.csv\")\n",
    "features = ['Pclass', 'Survived','Age_Imputed', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'female']\n",
    "\n",
    "#Keeping relevant data for processing \n",
    "train_data = train_data[features]\n",
    "\n",
    "#Converting dataset into array for Cross validation\n",
    "array = train_data.values\n",
    "\n",
    "#Seperating target variable and indepentdent variables\n",
    "X=np.delete(array, 1, axis=1)\n",
    "Y=array[:,1]\n",
    "\n",
    "#Setting the test size and train size\n",
    "test_size = 0.20\n",
    "seed = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "scoring = 'accuracy'\n",
    "models=SVC()\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "cv_results = model_selection.cross_val_score(models, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "results=(cv_results)\n",
    "accuracy_train = cv_results.mean()\n",
    "print(accuracy_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l2",
     "content",
     "s2"
    ]
   },
   "source": [
    "We should now predict the model on our test data.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "* Predict on the test data and find the accuracy of the model. Put the accuracy of the model in a variable called accuracy_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "l2",
     "ce",
     "s2"
    ]
   },
   "outputs": [],
   "source": [
    "# Make predictions on test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "use svm.fit(..) to fit the model and then accuracy_score(..) to find the accuracy\n",
    "\n",
    "```python\n",
    "svm.fit(X_train, Y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "accuracy_test= accuracy_score(Y_test, predictions)\n",
    "print(accuracy_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC\n",
    "\n",
    "Support Vector Machines has two options for Linear Model.\n",
    "\n",
    "<li>LinearSVC() \n",
    "<li>SVC(kernel='linear')\n",
    "\n",
    "\n",
    "The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences:\n",
    "\n",
    "LinearSVC uses the One-vs-Rest multiclass reduction while SVC (linear kernel) uses the One-vs-One multiclass reduction.\n",
    "\n",
    "\n",
    "In this section, let us use IRIS dataset with Comparison of two linear SVM classifiers. We only consider the first 2 features of this dataset:\n",
    "\n",
    "<b>Sepal length</b> <br>\n",
    "<b>Sepal width</b> <br>\n",
    "\n",
    "## Exercise\n",
    "\n",
    "* Use LinearSVC and SVC(kernel='linear') to fit IRIS dataset on 'Sepal length' and 'Sepal width'. Then print the accuracies for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "l2",
     "ce",
     "s2"
    ]
   },
   "outputs": [],
   "source": [
    "# Make predictions on test dataset\n",
    "# Make predictions on test dataset\n",
    "from sklearn import datasets,metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_data = iris.data\n",
    "iris_data = pd.DataFrame(iris_data, columns=iris.feature_names)\n",
    "iris_data['species'] = iris.target \n",
    "iris_data['species'].unique()\n",
    "\n",
    "features = iris.feature_names\n",
    "target = 'species'\n",
    "\n",
    "X = iris_data[features]\n",
    "y = iris_data[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "lsvm=LinearSVC()\n",
    "lsvm.fit(X_train, Y_train)\n",
    "predictions = lsvm.predict(X_test)\n",
    "accuracy_test= accuracy_score(Y_test, predictions)\n",
    "print('Accuracy of LinearSVC',accuracy_test)\n",
    "\n",
    "l_svc=SVC(kernel='linear')\n",
    "l_svc.fit(X_train, Y_train)\n",
    "predictions = l_svc.predict(X_test)\n",
    "accuracy_test= accuracy_score(Y_test, predictions)\n",
    "print('Accuracy of SVC with Linear kernel',accuracy_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Support Vector models\n",
    "\n",
    "Both linear models LinearSVC and SVC(kernel='linear') have linear decision boundaries - a hyperplane.\n",
    "\n",
    "The non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters. This is based on higher dimension.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "* Use non-linear kernel  SVC(kernel='poly') and SVC(kernel='rbf') to fit IRIS dataset on 'Sepal length' and 'Sepal width'. Then print the accuracies for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "l2",
     "ce",
     "s2"
    ]
   },
   "outputs": [],
   "source": [
    "# Make predictions on test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "\n",
    "```python\n",
    "p_svc=SVC(kernel='poly')\n",
    "p_svc.fit(X_train, Y_train)\n",
    "predictions = p_svc.predict(X_test)\n",
    "accuracy_test= accuracy_score(Y_test, predictions)\n",
    "print('Accuracy of SVC with Poly kernel',accuracy_test)\n",
    "\n",
    "r_svc=SVC(kernel='rbf')\n",
    "r_svc.fit(X_train, Y_train)\n",
    "predictions = r_svc.predict(X_test)\n",
    "accuracy_test= accuracy_score(Y_test, predictions)\n",
    "print('Accuracy of SVC with Poly kernel',accuracy_test)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "executed_sections": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
